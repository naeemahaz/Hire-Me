{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field of study focused on making sense or language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#what exactly are regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('abc', 'abcdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='hi'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_regex ='\\w+'\n",
    "re.match(word_regex, 'hi there!')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\w+ matches= word\n",
    "\\d matches digit\n",
    "\\s matches space\n",
    "* wildcard \n",
    "+ or * matches greedy match\n",
    "\\S matches not space\n",
    "[a-z] matches lowercase group "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "re module \n",
    "split - split a string on regex\n",
    "findall - find all patterns in a string \n",
    "search - search for a pattern \n",
    "match - match an entire string or substring based on a pattern \n",
    "\n",
    "\n",
    "Patten first, and the string second \n",
    "May return an iterator, string or match object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'on', 'spaces.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', 'Split on spaces.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Turning a string or document int tokens (smaller chucks)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "df = pd.DataFrame({'frases': ['Do not let the day end without having grown a little,', 'without having been happy, without having increased your dreams', 'Do not let yourself be overcomed by discouragement.','We are passion-full beings.']})\n",
    "\n",
    "df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['frases']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "Text preprocessing is a step that occurs after text mining. Text data can be sourced from difference places; text can come from online books, text can be web scraped and it may also come from online documentation. Text preprocessing is essential in order to further manipulate your text data. In natural language processing, one thing to keep in mind is that whatever you do to the raw data may have an impact on how your model will be trained. For example, stripping punctuation and removing upper cases may change the meaning of your sentences. This is something to keep in mind while going through your data and what you want to have as an end result.\n",
    "\n",
    "\n",
    "\n",
    "Imports\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NLTK is a suite of libraries which will help tokenize (break down) text into desired pieces of information (words and sentences). The nltk.stem package will allow for stemming and lemmatization (normalization techniques). Both NumPy and Pandas are imported in case you have a preference when manipulating your data. If you would like more information about NumPy and Pandas, click here. Finally, regular expressions are imported to help us identify/remove specific characters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Import Data\n",
    "\n",
    "\n",
    "Df_pd = pd.read_csv(\"C:/Your/files/textdata.csv\",encoding = 'utf-8', header = None)\n",
    "Df_np = np.asarray(pd.read_csv(\"C:/Your/files/textdata.csv\", encoding ='utf-8' header = None)\n",
    "\n",
    "\n",
    "Two options are shown here to import your file into python. Although they look the same since they both utilize pd.read_csv, the manipulations will be different further downstream. The encoding in this example is utf-8, this encoding is good to follow when processing text which is taken from online sources since you may be dealing with more than 256 possible characters ( 256 is the limit of the single-byte latin-1 encoding). If you do not know what encoding your file was saved in, please click here."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sentence Tokenization\n",
    "The meaning of tokenization is to chop up some existing text into smaller chunks. For example, a paragraph can be tokenized into sentences and further into words. We will first start with a simple string that we would like to divide into sentences.\n",
    "\n",
    "\n",
    "paragraph = \" Hello world! I am going for coffee before work! \"\n",
    "sentences = sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Regular Expressions\n",
    "This function will return a clean version of your text. For people using panda, each row will contain a clean version of the text that was in each row. For NumPy users, the function will return a clean text element in the array. If you need more information on regular expressions, click here.\n",
    "\n",
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in (text[:][0]): #this is Df_pd for Df_np (text[:])\n",
    "        new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punc.\n",
    "        new_text = re.sub(r'\\d+','',new_text)# remove numbers\n",
    "        new_text = new_text.lower() # lower case, .upper() for upper          \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word Tokenization\n",
    "Same principal applies as the sentence tokenizer, here we use word_tokenize from the nltk.tokenize package. First we will tokenize words from a simple string.\n",
    "\n",
    "sentences = \" Hello world! I am going for coffee before work! \"\n",
    "words = word_tokenize(sentences)\n",
    "print(words)\n",
    "\n",
    "It is important to consider in which order you perform your preprocessing steps; the preprocess function was utilized before the word tokenization function in order to avoid punctuation to be stored as a word. If your data is stored in pandas you will want to create a function to loop over every row in your data frame.\n",
    "\n",
    "\n",
    "def tokenization_w(words):\n",
    "    w_new = []\n",
    "    for w in (words[:][0]):  # for NumPy = words[:]\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            w_new.append(w_token)\n",
    "    return w_new\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Stemming\n",
    "Stemming is used to normalize parts of text data. What does this mean exactly? When you are using a verb which is conjugated in multiple tenses throughout a document you would like to process, stemming will shorten all of these conjugated verbs to the shortest length of characters possible; it will preserve the root of the verb in this case. Stemming is done for all types of words, adjectives and more (which have the same root).\n",
    "In this tutorial we will use the SnowBallStemmer from the nltk.stem package. Stemming can also be achieved with the PorterStemmer. This example is once again performed on a panda frame.\n",
    "\n",
    "snowball = SnowballStemmer(language = 'english')\n",
    "def stemming(words):\n",
    "    new = []\n",
    "    stem_words = [snowball.stem(x) for x in (words[:][0])]\n",
    "    new.append(stem_words)\n",
    "    return new\n",
    "    \n",
    "    \n",
    "Let’s run the function to show the output of this function:\n",
    "What we can observe is no punctuation, no upper cases and each word is stored individually (tokenized). Stemming occurred on the verb “to eat” , the word because and apple.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lemmatization\n",
    "Lemmatization is another normalization technique which is used in natural language processing. The linguistic difference with respect to stemming is that lemmatization will enable for words which do not have the same root to be grouped together in order for them to be processed as one item.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(words):\n",
    "    new = []\n",
    "    lem_words = [lemmatizer.lemmatize(x) for x in (words[:][0])]\n",
    "    new.append(lem_words)\n",
    "    return new\n",
    "    \n",
    "If we use test as our input variable for the lemmatization function we will see what has been grouped together and how the result differs from stemming.\n",
    "\n",
    "lemtest = lemmatization(clean_words)\n",
    "print(lemtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('Hi there!')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sent_tokenize: tokenize a document into sentences\n",
    "\n",
    "regexp_tokenize: tokenize a string or document based on a regular expression\n",
    "\n",
    "TweetTokenizer: Special class just for tweet tokenization, allowing you to separate hashtags, mentions and lots of exclamation points    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.match('abc','abcde')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('abc', 'abcde')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match('cd', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(2, 4), match='cd'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('cd', 'abcde')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_digits_and_word = ('(\\d+|\\w+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\\\\d+|\\\\w+)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_digits_and_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(match_digits_and_word, 'He has 11 cats.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[A-Za-z]+ mataches = upper and lowercase English\n",
    "[0-9]  matches = numbers from 0 to 9\n",
    "[A-Za-z\\-\\]+ matches = upper and lowercaase English alphabet\n",
    "(a-z) matches = a, - and z \n",
    "(/s+|,) matches spaces or a comma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 5), match='match'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "\n",
    "re.match('[a-z0-9]+', my_str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)\n",
    "\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)\n",
    "\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charting word length with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAPPklEQVR4nO3da4wdd33G8e9DbC4Jl1Txtri+sFREqIAKSVcmNFIUEagSEiW9pJIjlZuoXKHQJi1SFXgRBK9AqqCCICIXpyQ0BGgSkEvMJRVQ4EUMa+NcHIPq0kCWuI0h4MSFEkx/fXHGaHuy63PWPptZ//l+pCPPnPnvzJPV+sns/8yMU1VIkk5+T+k7gCRpMix0SWqEhS5JjbDQJakRFrokNWJVXwdes2ZNTU9P93V4STop7dq16wdVNbXQtt4KfXp6mtnZ2b4OL0knpSTfXWybUy6S1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpESMLPcnTk3w9yd1J9iZ55wJjnpbkE0n2J9mZZHo5wkqSFjfOGfrPgFdW1UuBlwEXJjlnaMybgB9V1QuA9wHvmWxMSdIoIwu9Bg53q6u71/BD1C8DbuyWbwUuSJKJpZQkjTTWnaJJTgF2AS8APlhVO4eGrAMeBKiqI0kOAWcAPxjazxZgC8DGjRtPLLmkiZm+5o7ejv3Auy/u7ditGetD0ar6RVW9DFgPbErykqEhC52NP+GfQqqqrVU1U1UzU1MLPopAknSclnSVS1X9GPgycOHQpjlgA0CSVcBzgEcmkE+SNKZxrnKZSnJ6t/wM4FXAt4aGbQde3y1fDnyx/MdKJelJNc4c+lrgxm4e/SnAJ6vqM0neBcxW1XZgG/DRJPsZnJlvXrbEkqQFjSz0qroHOGuB96+dt/w/wJ9MNpokaSm8U1SSGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGjGy0JNsSPKlJPuS7E1y1QJjzk9yKMme7nXt8sSVJC1m1RhjjgBvrardSZ4F7EpyZ1XdPzTuq1V1yeQjSpLGMfIMvaoOVNXubvkxYB+wbrmDSZKWZklz6EmmgbOAnQtsfkWSu5N8NsmLF/n6LUlmk8wePHhwyWElSYsbu9CTPBO4Dbi6qh4d2rwbeF5VvRT4APDphfZRVVuraqaqZqampo43syRpAWMVepLVDMr85qq6fXh7VT1aVYe75R3A6iRrJppUknRM41zlEmAbsK+q3rvImOd240iyqdvvDycZVJJ0bONc5XIu8Frg3iR7uvfeDmwEqKrrgcuBNyc5AvwU2FxVtQx5JUmLGFnoVfU1ICPGXAdcN6lQkqSl805RSWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjRhZ6Ek2JPlSkn1J9ia5aoExSfL+JPuT3JPk7OWJK0lazKoxxhwB3lpVu5M8C9iV5M6qun/emIuAM7vXy4EPdX9Kkp4kI8/Qq+pAVe3ulh8D9gHrhoZdBtxUA3cBpydZO/G0kqRFjXOG/ktJpoGzgJ1Dm9YBD85bn+veOzD09VuALQAbN25cWlLpSTR9zR29HPeBd1/cy3HVhrE/FE3yTOA24OqqenR48wJfUk94o2prVc1U1czU1NTSkkqSjmmsQk+ymkGZ31xVty8wZA7YMG99PfDQiceTJI1rnKtcAmwD9lXVexcZth14XXe1yznAoao6sMhYSdIyGGcO/VzgtcC9SfZ0770d2AhQVdcDO4DXAPuBnwBvnHxUSdKxjCz0qvoaC8+Rzx9TwJWTCiVJWjrvFJWkRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRows9CQ3JHk4yX2LbD8/yaEke7rXtZOPKUkaZdUYYz4CXAfcdIwxX62qSyaSSJJ0XEaeoVfVV4BHnoQskqQTMKk59FckuTvJZ5O8eLFBSbYkmU0ye/DgwQkdWpIEkyn03cDzquqlwAeATy82sKq2VtVMVc1MTU1N4NCSpKNOuNCr6tGqOtwt7wBWJ1lzwskkSUtywoWe5LlJ0i1v6vb5wxPdryRpaUZe5ZLkFuB8YE2SOeAdwGqAqroeuBx4c5IjwE+BzVVVy5ZYkrSgkYVeVVeM2H4dg8saJUk98k5RSWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjRhZ6EluSPJwkvsW2Z4k70+yP8k9Sc6efExJ0ijjnKF/BLjwGNsvAs7sXluAD514LEnSUo0s9Kr6CvDIMYZcBtxUA3cBpydZO6mAkqTxrJrAPtYBD85bn+veOzA8MMkWBmfxbNy48bgPOH3NHcf9tSfqgXdf3NuxJU1Oiz0yiQ9Fs8B7tdDAqtpaVTNVNTM1NTWBQ0uSjppEoc8BG+atrwcemsB+JUlLMIlC3w68rrva5RzgUFU9YbpFkrS8Rs6hJ7kFOB9Yk2QOeAewGqCqrgd2AK8B9gM/Ad64XGElSYsbWehVdcWI7QVcObFEkqTj4p2iktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktSIsQo9yYVJvp1kf5JrFtj+hiQHk+zpXn82+aiSpGNZNWpAklOADwKvBuaAbyTZXlX3Dw39RFW9ZRkySpLGMM4Z+iZgf1V9p6oeBz4OXLa8sSRJSzVOoa8DHpy3Pte9N+yPk9yT5NYkGxbaUZItSWaTzB48ePA44kqSFjNOoWeB92po/Z+B6ar6HeBfgBsX2lFVba2qmaqamZqaWlpSSdIxjVPoc8D8M+71wEPzB1TVD6vqZ93q3wO/O5l4kqRxjVPo3wDOTPL8JE8FNgPb5w9Isnbe6qXAvslFlCSNY+RVLlV1JMlbgM8DpwA3VNXeJO8CZqtqO/CXSS4FjgCPAG9YxsySpAWMLHSAqtoB7Bh679p5y28D3jbZaJKkpfBOUUlqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY0Yq9CTXJjk20n2J7lmge1PS/KJbvvOJNOTDipJOraRhZ7kFOCDwEXAi4ArkrxoaNibgB9V1QuA9wHvmXRQSdKxjXOGvgnYX1XfqarHgY8Dlw2NuQy4sVu+FbggSSYXU5I0yqoxxqwDHpy3Pge8fLExVXUkySHgDOAH8wcl2QJs6VYPJ/n28YQG1gzv+8mSY//u0VuuMazUbOaaZ8TPFzT4/Rrjv/lErMjvV95zQrmet9iGcQp9oTPtOo4xVNVWYOsYxzx2oGS2qmZOdD+TtlJzwcrNZq6lMdfS/KrlGmfKZQ7YMG99PfDQYmOSrAKeAzwyiYCSpPGMU+jfAM5M8vwkTwU2A9uHxmwHXt8tXw58saqecIYuSVo+I6dcujnxtwCfB04BbqiqvUneBcxW1XZgG/DRJPsZnJlvXs7QTGDaZpms1FywcrOZa2nMtTS/UrniibQktcE7RSWpERa6JDXipCr0JDckeTjJfX1nmS/JhiRfSrIvyd4kV/WdCSDJ05N8PcndXa539p1pviSnJPlmks/0neWoJA8kuTfJniSzfec5KsnpSW5N8q3u5+wVKyDTC7vv09HXo0mu7jsXQJK/6n7m70tyS5Kn950JIMlVXaa9y/G9Oqnm0JOcBxwGbqqql/Sd56gka4G1VbU7ybOAXcAfVNX9PecKcFpVHU6yGvgacFVV3dVnrqOS/DUwAzy7qi7pOw8MCh2YqaoVdTNKkhuBr1bVh7urzU6tqh/3neuo7hEh3wdeXlXf7TnLOgY/6y+qqp8m+SSwo6o+0nOulzC4034T8DjwOeDNVfVvkzrGSXWGXlVfYQVe315VB6pqd7f8GLCPwd2zvaqBw93q6u61Iv4PnmQ9cDHw4b6zrHRJng2cx+BqMqrq8ZVU5p0LgH/vu8znWQU8o7sv5lSeeO9MH34buKuqflJVR4B/Bf5wkgc4qQr9ZNA9afIsYGe/SQa6aY09wMPAnVW1InIBfwf8DfC/fQcZUsAXkuzqHlWxEvwWcBD4h26K6sNJTus71JDNwC19hwCoqu8Dfwt8DzgAHKqqL/SbCoD7gPOSnJHkVOA1/P+bNk+YhT5BSZ4J3AZcXVWP9p0HoKp+UVUvY3CH76bu175eJbkEeLiqdvWdZQHnVtXZDJ4uemU3zde3VcDZwIeq6izgv4EnPMa6L90U0KXAP/WdBSDJrzF4YODzgd8ETkvyp/2mgqrax+BJtHcymG65GzgyyWNY6BPSzVHfBtxcVbf3nWdY9yv6l4ELe44CcC5waTdf/XHglUn+sd9IA1X1UPfnw8CnGMx39m0OmJv329WtDAp+pbgI2F1V/9V3kM6rgP+oqoNV9XPgduD3es4EQFVtq6qzq+o8BtPHE5s/Bwt9IroPH7cB+6rqvX3nOSrJVJLTu+VnMPhB/1a/qaCq3lZV66tqmsGv6l+sqt7PoJKc1n2oTTel8fsMfk3uVVX9J/Bgkhd2b10A9PqB+5ArWCHTLZ3vAeckObX7u3kBg8+1epfk17s/NwJ/xIS/b+M8bXHFSHILcD6wJskc8I6q2tZvKmBwxvla4N5uvhrg7VW1o8dMAGuBG7srEJ4CfLKqVswlgivQbwCf6h7lvwr4WFV9rt9Iv/QXwM3d9MZ3gDf2nAeAbi741cCf953lqKrameRWYDeDKY1vsnIeAXBbkjOAnwNXVtWPJrnzk+qyRUnS4pxykaRGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpEf8Hm+oDInLF3XYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist([1,5,5,7,7,7,9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 0., 1., 0., 0., 0., 3., 0., 0., 1.]),\n",
       " array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(\"This is a pretty cool tool!\")\n",
    "word_lengths = [len(w) for w in words]\n",
    "plt.hist(word_lengths)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word counts with bag-of-words\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We need a way to represent text data for machine learning algorithm and the bag-of-words model helps us to achieve that task. The bag-of-words model is simple to understand and implement. It is a way of extracting features from the text for use in machine learning algorithms."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The process of converting NLP text into numbers is called vectorization in ML. Different ways to convert text into vectors are:\n",
    "Counting the number of times each word appears in a document.\n",
    "Calculating the frequency that each word appears in a document out of all the words in the document.\n",
    "\n",
    "CountVectorizer\n",
    "CountVectorizer works on Terms Frequency, i.e. counting the occurrences of tokens and building a sparse matrix of documents x tokens.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF Vectorizer\n",
    "TF-IDF stands for term frequency-inverse document frequency. TF-IDF weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "Term Frequency (TF): is a scoring of the frequency of the word in the current document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. The term frequency is often divided by the document length to normalize.\n",
    "\n",
    "\n",
    "TF(t) = (Numberof times term t appears in a document)/(Total number of terms in the document)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Inverse Document Frequency (IDF): is a scoring of how rare the word is across documents. IDF is a measure of how rare a term is. Rarer the term, more is the IDF score\n",
    "\n",
    "IDF(t) = loge(Total number of documents/Number of documents with term t in it)\n",
    "\n",
    "Thus, \n",
    "\n",
    "TF-IDFscore = TF*IDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step #1 : We will first preprocess the data, in order to:\n",
    "\n",
    "Convert text to lower case.\n",
    "Remove all non-word characters.\n",
    "Remove all punctuations.\n",
    "\n",
    "# Python3 code for preprocessing text \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "  \n",
    "# execute the text here as : \n",
    "# text = \"\"\" # place text here  \"\"\" \n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can further preprocess the text to suit you needs.\n",
    "Step #2 : Obtaining most frequent words in our text.\n",
    "\n",
    "We will apply the following steps to generate our model.\n",
    "\n",
    "We declare a dictionary to hold our bag of words.\n",
    "Next we tokenize each sentence to words.\n",
    "Now for each word in sentence, we check if the word exists in our dictionary.\n",
    "If it does, then we increment its count by 1. If it doesn’t, we add it to our dictionary and set its count as 1.\n",
    "\n",
    "\n",
    "# Creating the Bag of Words model \n",
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1\n",
    "            \n",
    "            \n",
    "In our model, we have a total of 118 words. However when processing large texts, the number of words could reach millions. We do not need to use all those words. Hence, we select a particular number of most frequently used words. To implement this we use:\n",
    "\n",
    "\n",
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get) \n",
    "\n",
    "\n",
    "where 100 denotes the number of words we want. If our text is large, we feed in a larger number."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step #3 : Building the Bag of Words model\n",
    "In this step we construct a vector, which would tell us whether a word in each sentence is a frequent word or not. If a word in a sentence is a frequent word, we set it as 1, else we set it as 0.\n",
    "This can be implemented with the help of following code:\n",
    "    \n",
    "    \n",
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X) \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Introduction\n",
    "Bag of Words (BoW) is a model used in natural language processing. One aim of BoW is to categorize documents. The idea is to analyse and classify different “bags of words” (corpus). And by matching the different categories, we identify which “bag” a certain block of text (test data) comes from.\n",
    "\n",
    "Putting into context\n",
    "One excellent way to explain this is to put this model into content. One classic use of BoW is for spam filtering. Through the use of the BoW model, the system is trained to differentiate between spam and ham (actual message). To extend the metaphor, we are trying to guess which bag the document comes from, the “bag of spam” or the “bag of ham”.\n",
    "\n",
    "Note: I will not be explaining the logic behind how the spam filter works (though I might do it in a different post). I am just giving the example so you can understand the rationale of categorizing different text.\n",
    "\n",
    "https://ongspxm.github.io/blog/2014/12/bag-of-words-natural-language-processing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 2,\n",
       "         'box': 3,\n",
       "         '.': 3,\n",
       "         'likes': 1,\n",
       "         'th': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes th box.  The box is over the cat.\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter.most_common(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bag-of-words picker\n",
    "It's time for a quick check on your understanding of bag-of-words. Which of the below options, with basic nltk tokenization, map the bag-of-words for the following text?\n",
    "\n",
    "\"The cat is in the box. The cat box.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Artificial intelligence (AI) is the most abstract one. It is a science that studies human brain and uses that outcome as a study basis for intelligent systems. It is not related only to computer science, but also to mathematics, psychology, philosophy and many others.\n",
    "\n",
    "Machine learning (ML)is a subset of AI at its most basic is the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world. So rather than hand-coding software routines with a specific set of instructions to accomplish a particular task, the machine is “trained” using large amounts of data and algorithms that give it the ability to learn how to perform the task.\n",
    "\n",
    "Deep learning (DL) is a machine learning discipline. It just stands for some types of neural network algorithms that use raw data and describe complex neural networks with many levels of abstraction.\n",
    "\n",
    "Natural Language Processing (NLP) is also one of the AI disciplines (not only AI). Machine learning algorithms are often used to solve NLP tasks. It is the ability of a computer to “understand” and generate human language. If you have a lot of data written in plain text and you want to automatically get some insights from it, you need to use NLP techniques. These insights could be sentiment analysis, information extraction, information retrieval, search etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Online courses\n",
    "Dan Jurafsky & Chris Manning: Natural Language Processing [great introductory video series]\n",
    "Stanford CS224d: Deep Learning for Natural Language Processing [more advanced ML algorithms, deep learning, and NN architectures for NLP]\n",
    "Coursera: Introduction to Natural Language Processing [intro NLP course offered by the University of Michigan]\n",
    "\n",
    "\n",
    "\n",
    "Libraries and open source\n",
    "spaCy (website, blog) [Python; emerging open-source library with fantastic usage examples, API documentation, and demo applications]\n",
    "Natural Language Toolkit (NLTK) (website, book) [Python; practical intro to programming for NLP, mainly used for teaching]\n",
    "Stanford CoreNLP (website) [Java; high-quality analysis toolkit]\n",
    "AllenNLP (website) [Python; NLP research library built on PyTorch]\n",
    "fastText (website) [C++; efficient library for text classification and representation learning]\n",
    "\n",
    "\n",
    "Active blogs\n",
    "natural language processing blog (Hal Daumé III)\n",
    "Language Log (Mark Liberman)\n",
    "Google Research blog\n",
    "Explosion AI blog\n",
    "Hugging Face\n",
    "Sebastian Ruder’s blog\n",
    "Books\n",
    "Speech and Language Processing (Jurafsky and Martin) [classic NLP textbook that covers all the basics, 3rd edition coming soon]\n",
    "Foundations of Statistical Natural Language Processing (Manning and Schütze) [more advanced, statistical NLP methods]\n",
    "Introduction to Information Retrieval (Manning, Raghavan and Schütze) [excellent reference on ranking/search]\n",
    "Neural Network Methods in Natural Language Processing (Goldberg) [deep intro to NN approaches to NLP, primer here]\n",
    "Linguistic Fundamentals for Natural Language Processing (Bender) [morphology and syntax basics for more successful NLP]\n",
    "Deep Learning (Goodfellow, Courville and Bengio) [best intro to deep learning]\n",
    "Miscellaneous\n",
    "How to build a word2vec model in TensorFlow [tutorial]\n",
    "Deep Learning for NLP resources [overview of state-of-the-art resources for deep learning, organized by topic]\n",
    "Last Words: Computational Linguistics and Deep Learning — A look at the importance of Natural Language Processing. (Manning) [article]\n",
    "Natural Language Understanding with Distributed Representation (Cho) [self-contained lecture note on ML/NN approaches to NLU]\n",
    "Bayesian Inference with Tears (Knight) [tutorial workbook]\n",
    "Association for Computational Linguistics (ACL) [journal anthology]\n",
    "Quora: How do I learn Natural Language Processing?\n",
    "Natural Language Understanding and Computational Semantics (Bowman) [open-source course syllabus with comprehensive slides]\n",
    "fast.ai [“Making neural nets uncool again”]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple text preprocessing\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Preprocessing steps like tokenization or lowercasing words are commonly used in NLP. Othe common techniques are things like lemmatization or stemming, where you shorten the words to their roots stems, or techniques like removing stop words, which are common words in a language. Removing punctuation or unwanted tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text =\"\"\"The cat is in the box.  The cat likes the box. The box is over the cat.\"\"\"\n",
    "\n",
    "tokens = [w for w in word_tokenize(text.lower())\n",
    "         if w.isalpha()]\n",
    "\n",
    "no_stops = [t for t in tokens\n",
    "           if t not in stopwords.words('english')]\n",
    "\n",
    "Counter(no_stops).most_common(2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to gensim\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gensim is billed as a Natural Language Processing package that does ‘Topic Modeling for Humans’. But its practically much more than that.\n",
    "\n",
    "If you are unfamiliar with topic modeling, it is a technique to extract the underlying topics from large volumes of text. Gensim provides algorithms like LDA and LSI (which we will see later in this post) and the necessary sophistication to build high-quality topic models.\n",
    "\n",
    "Uses top academic models to perform complex tasks\n",
    "\n",
    "Building document or word vectors and performing topic identification and document comparison "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gensim = “Generate Similar” is a popular open source natural language processing (NLP) library used for unsupervised topic modeling. It uses top academic models and modern statistical machine learning to perform various complex tasks such as −\n",
    "\n",
    "Building document or word vectors\n",
    "Corpora\n",
    "Performing topic identification\n",
    "Performing document comparison (retrieving semantically similar documents)\n",
    "Analysing plain-text documents for semantic structure\n",
    "Apart from performing the above complex tasks, Gensim, implemented in Python and Cython, is designed to handle large text collections using data streaming as well as incremental online algorithms. This makes it different from those machine learning software packages that target only in-memory processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses of Gensim\n",
    "Gensim has been used and cited in over thousand commercial and academic applications. It is also cited by various research papers and student theses. It includes streamed parallelised implementations of the following −\n",
    "\n",
    "fastText\n",
    "fastText, uses a neural network for word embedding, is a library for learning of word embedding and text classification. It is created by Facebook’s AI Research (FAIR) lab. This model, basically, allows us to create a supervised or unsupervised algorithm for obtaining vector representations for words.\n",
    "\n",
    "Word2vec\n",
    "Word2vec, used to produce word embedding, is a group of shallow and two-layer neural network models. The models are basically trained to reconstruct linguistic contexts of words.\n",
    "\n",
    "LSA (Latent Semantic Analysis)\n",
    "It is a technique in NLP (Natural Language Processing) that allows us to analyse relationships between a set of documents and their containing terms. It is done by producing a set of concepts related to the documents and terms.\n",
    "\n",
    "LDA (Latent Dirichlet Allocation)\n",
    "It is a technique in NLP that allows sets of observations to be explained by unobserved groups. These unobserved groups explain, why some parts of the data are similar. That’s the reason, it is a generative statistical model.\n",
    "\n",
    "tf-idf (term frequency-inverse document frequency)\n",
    "tf-idf, a numeric statistic in information retrieval, reflects how important a word is to a document in a corpus. It is often used by search engines to score and rank a document’s relevance given a user query. It can also be used for stop-words filtering in text summarisation and classification.\n",
    "\n",
    "All of them will be explained in detail in the next sections.\n",
    "\n",
    "Advantages\n",
    "Gensim is a NLP package that does topic modeling. The important advantages of Gensim are as follows −\n",
    "\n",
    "We may get the facilities of topic modeling and word embedding in other packages like ‘scikit-learn’ and ‘R’, but the facilities provided by Gensim for building topic models and word embedding is unparalleled. It also provides more convenient facilities for text processing.\n",
    "\n",
    "Another most significant advantage of Gensim is that, it let us handle large text files even without loading the whole file in memory.\n",
    "\n",
    "Gensim doesn’t require costly annotations or hand tagging of documents because it uses unsupervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a gensim dictionary "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from gensim.corpora.dictionary import Dictionary \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)\n",
    "#> Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Show the word to id map\n",
    "print(dictionary.token2id)\n",
    "#> {'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, \n",
    "#> 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \n",
    "#> \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, \n",
    "#> 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, \n",
    "#> 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, \n",
    "#> 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, \n",
    "#> 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have successfully created a Dictionary object. Gensim will use this dictionary to create a bag-of-words corpus where the words in the documents are replaced with its respective id provided by this dictionary.\n",
    "\n",
    "If you get new documents in the future, it is also possible to update an existing dictionary to include the new words."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creating and querying a corpus with gensim\n",
    "It's time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus!\n",
    "\n",
    "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You'll need to do some light preprocessing and then generate the gensim dictionary and corpus.                    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gensim bag-of-words\n",
    "Now, you'll use your new gensim corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!\n",
    "\n",
    "You have access to the dictionary and corpus objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis.\n",
    "\n",
    "defaultdict allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int, we are able to ensure that any non-existent keys are automatically assigned a default value of 0. This makes it ideal for storing the counts of words in this exercise.\n",
    "\n",
    "itertools.chain.from_iterable() allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists).\n",
    "\n",
    "The fifth document from corpus is stored in the variable doc, which has been sorted in descending order."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count \n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf with gensim\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Term frequency - inverse document frequency\n",
    "Allows you to determine the most important words in each document \n",
    "The idea behind tf-idf is that each corpus might have more shared words than just stopwords\n",
    "These common words are like stopwords and should be removed or at least down weighted in importance \n",
    "\n",
    "Ensures most common words don't show up as key words \n",
    "\n",
    "Keeps document specific frequent words wighted high "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What does tf-idf mean?\n",
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
    "\n",
    "Tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.\n",
    "\n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF (Term Frequency) measures the frequency of a word in a document.\n",
    "TF = (Number of time the word occurs in the text) / (Total number of words in text)\n",
    "\n",
    "\n",
    "IDF (Inverse Document Frequency) measures the rank of the specific word for its relevancy within the text. Stop words which contain unnecessary information such as “a”, “into” and “and” carry less importance in spite of their occurrence.\n",
    "IDF = (Total number of documents / Number of documents with word t in it)\n",
    "\n",
    "Thus, the TF-IDF is the product of TF and IDF:\n",
    "TF-IDF = TF * IDF    \n",
    "\n",
    "\n",
    "In order to acquire good results with TF-IDF, a huge corpus is necessary. In my example, I just used a small sized corpus. Since I removed stop words, result was pleasant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my previous code piece, we start again by adding modules to use their methods. In this example, we utilize Scikit-learn besides Numpy, Pandas and Regular Expression. Scikit-learn is a free machine learning library for python. We will utilize CountVectorizer to convert a collection of text documents to a matrix of token counts. TfidfTransformers handles transformation of a count matrix to a normalized TF or TF-IDF representation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentences = list()\n",
    "with open(\"resources/beatles_biography\") as file:\n",
    "    for line in file:\n",
    "        for l in re.split(r\"\\.\\s|\\?\\s|\\!\\s|\\n\",line):\n",
    "            if l:\n",
    "                sentences.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is fetched from ‘beatles_biography’ file and we are parse the text in order to obtain sentences. Regular expression helps separation of sentences using marks then sentences are enlisted under sentences object."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cvec = CountVectorizer(stop_words='english', min_df=3, max_df=0.5, ngram_range=(1,2))\n",
    "sf = cvec.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 4 parameters in CountVectorizer method. First one is stop_words which removes words that occur a lot but do not contain necessary information. ‘None’ can be given if we don’t want to remove any word or we can give a list to choose which words are going to be swept ourselves. In Scikit-learn, English stop word list is provided built-in. min_df parameter is a threshold value where we ignore terms that have a document frequency lower than min_df. max_df is the contrast of min_df parameter. If the document frequency of a word is more than max_df, we ignore it. ngram_range(x,y) is the last parameter which defines the boundary of n values for different n-grams. x is for the minimum n value, y represents the maximum n value for n-grams. fit_transform returns transform version of sentences."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(sf)\n",
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform a count matrix to a normalized TF or TF-IDF representation to measure weights. As I mentioned above, the word which has the highest weight provides more information about the document. At the end of the transformation, list is acquired which comprises terms and their ranks."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weights_df.sort_values(by='weight', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can print top 10 words through the given document.\n",
    "\n",
    "TF-IDF is a numerical statistic used in information retrieval and text mining. I wanted share my experience on it. You can find full code from my repository. In our next article we are going to continue with implementing word2vec to make relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf with gensim"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NLP task to identfy important named entities in the text - NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence ='''In New York, I like to ride the Metro to visit MOMA and some restaurants rated well by Ruth Reichl.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'New',\n",
       " 'York',\n",
       " ',',\n",
       " 'I',\n",
       " 'like',\n",
       " 'to',\n",
       " 'ride',\n",
       " 'the',\n",
       " 'Metro',\n",
       " 'to',\n",
       " 'visit',\n",
       " 'MOMA',\n",
       " 'and',\n",
       " 'some',\n",
       " 'restaurants',\n",
       " 'rated',\n",
       " 'well',\n",
       " 'by',\n",
       " 'Ruth',\n",
       " 'Reichl',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sent = nltk.pos_tag(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('like', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('ride', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Metro', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('visit', 'VB'),\n",
       " ('MOMA', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('some', 'DT'),\n",
       " ('restaurants', 'NNS'),\n",
       " ('rated', 'VBN'),\n",
       " ('well', 'RB'),\n",
       " ('by', 'IN'),\n",
       " ('Ruth', 'NNP'),\n",
       " ('Reichl', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sent #parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  ride/VB\n",
      "  the/DT\n",
      "  (ORGANIZATION Metro/NNP)\n",
      "  to/TO\n",
      "  visit/VB\n",
      "  (ORGANIZATION MOMA/NNP)\n",
      "  and/CC\n",
      "  some/DT\n",
      "  restaurants/NNS\n",
      "  rated/VBN\n",
      "  well/RB\n",
      "  by/IN\n",
      "  (PERSON Ruth/NNP Reichl/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NER with NLTK\n",
    "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use nltk to find the named entities in this article.\n",
    "\n",
    "What might the article be about, given the names you found?\n",
    "\n",
    "Along with nltk, sent_tokenize and word_tokenize from nltk.tokenize have been pre-imported."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Charting practice\n",
    "In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n",
    "\n",
    "You'll use a defaultdict called ner_categories, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called chunked_sentences similar to the last exercise, but this time with non-binary category names.\n",
    "\n",
    "You can use hasattr() to determine if each chunk has a 'label' and then simply use the chunk's .label() method as the dictionary key."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SpaCy\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spaCy (/speɪˈsiː/ spay-SEE) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.[3][4] The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\n",
    "\n",
    "Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage.[5][6] As of version 1.0, spaCy also supports deep learning workflows[7] that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, PyTorch or MXNet through its own machine learning library Thinc.[8][9] Using Thinc as its backend, spaCy features convolutional neural network models for part-of-speech tagging, dependency parsing, text categorization and named entity recognition (NER). Prebuilt statistical neural network models to perform these task are available for English, German, Greek, Spanish, Portuguese, French, Italian, Dutch, Lithuanian and Norwegian, and there is also a multi-language NER model. Additional support for tokenization for more than 50 languages allows users to train custom models on their own datasets as well.\n",
    "\n",
    "https://www.datacamp.com/community/blog/spacy-cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
